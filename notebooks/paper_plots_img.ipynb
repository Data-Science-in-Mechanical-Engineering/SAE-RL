{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import gymnasium\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from cycler import cycler\n",
    "from gymnasium.utils import seeding\n",
    "from hydra import compose, initialize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from omegaconf import DictConfig\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import __init__\n",
    "from scripts.train_rl import setup_environments\n",
    "from src.data import load\n",
    "from src.data.loading import ConstantRandomSampler\n",
    "from src.environments.utils import antialias\n",
    "from src.evaluation.utils import mm2in\n",
    "from src.metrics.transforms import AffineTransform, ScaleTranslateTransform\n",
    "from src.models.sae import assemble_sae\n",
    "from src.utils import Bunch, deflate, get_display, gl, inflate, print_cfg\n",
    "\n",
    "sys.modules['gym'] = gymnasium  # see [PR](https://github.com/DLR-RM/stable-baselines3/pull/780)\n",
    "from stable_baselines3 import SAC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Plotting setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'axes.prop_cycle': cycler('color', [\"#0173B2\", \"#DE8F05\", \"#029E73\", \"#D55E00\", \"#CC78BC\",\n",
    "                                        \"#CA9161\", \"#FBAFE4\", \"#949494\", \"#ECE133\", \"#56B4E9\"]),\n",
    "    'axes.titlepad': 3.0,\n",
    "    'axes.xmargin': 0.025,\n",
    "    'axes.ymargin': 0.025,\n",
    "    'axes.titlesize': 'medium',\n",
    "    'axes.labelpad': 1.0,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.top': False,\n",
    "    'font.family': 'serif',\n",
    "    'font.size': 8,\n",
    "    'text.usetex': True,\n",
    "    'text.latex.preamble': [r'\\usepackage{lmodern}'],\n",
    "    'grid.alpha': 0.1,\n",
    "    'grid.color': '#000000',\n",
    "    'legend.borderaxespad': 0.25,\n",
    "    'legend.borderpad': 0.0,\n",
    "    'legend.frameon': False,\n",
    "    'legend.columnspacing': 1.0,\n",
    "    'legend.handletextpad': 0.5,\n",
    "    'legend.handlelength': 0.75,\n",
    "    'lines.solid_capstyle': 'round',\n",
    "    'lines.solid_joinstyle': 'round',\n",
    "    'xtick.major.pad': 2.0,\n",
    "    'xtick.major.size': 2.0,\n",
    "    'xtick.minor.size': 1.0,\n",
    "    'ytick.major.pad': 2.0,\n",
    "    'ytick.major.size': 2.0,\n",
    "    'ytick.minor.size': 1.0,\n",
    "    'figure.constrained_layout.h_pad': 0.0,\n",
    "    'figure.constrained_layout.hspace': 0.0,\n",
    "    'figure.constrained_layout.use': True,\n",
    "    'figure.constrained_layout.w_pad': 0.0,\n",
    "    'figure.constrained_layout.wspace': 0.0\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with initialize(version_base=None, config_path='../configs'):\n",
    "    rl_cfg = compose(config_name='train_rl', overrides=[\n",
    "        '+experiment=rl-feat',\n",
    "        'training.observation.keypoints=True',\n",
    "        'training.sae_checkpoint=logs/sae/panda_push_custom/keynet+keynet/2023-03-16--02-11-09--34050912/checkpoint_final.pth',\n",
    "        'training.sae_name=autumn-resonance-526',\n",
    "        'training.sae_experiment=sae-keynet-vel-var-bg',\n",
    "        'wandb=off',\n",
    "        'hydra=hush'\n",
    "    ])\n",
    "\n",
    "with initialize(version_base=None, config_path='../configs'):\n",
    "    sae_cfg = compose(config_name='train_sae', overrides=[\n",
    "        '+experiment=sae-keynet-vel-var-bg',\n",
    "        'wandb=off',\n",
    "        'hydra=hush'\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = Bunch(**torch.load('../logs/sae/panda_push_custom/keynet+keynet/2023-03-16--02-11-09--34050912/checkpoint_final.pth', map_location=gl.device))\n",
    "\n",
    "# reinstantiate model and optimizer\n",
    "model = assemble_sae(sae_cfg)\n",
    "model.load_state_dict(checkpoint.model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "dataset_valid, = load(sae_cfg, valid=True)\n",
    "\n",
    "loader_valid = DataLoader(dataset_valid, sae_cfg.training.batch_size,\n",
    "                          sampler=ConstantRandomSampler(dataset_valid, sae_cfg.dataset.seed),\n",
    "                          shuffle=False, drop_last=True, num_workers=8, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()  # put model into evaluation state\n",
    "    track_fps = []  # feature points for tracking error computation\n",
    "    track_kps = []  # site coordinates for tracking error computation\n",
    "\n",
    "    # loop over validation batches\n",
    "    for batch, (inputs, _, sites) in enumerate(tqdm(loader_valid, leave=False)):\n",
    "\n",
    "        # move all data to GPU\n",
    "        inputs = inputs.to(gl.device)\n",
    "\n",
    "        # encoder pass to obtain feature points\n",
    "        fps = model.encoder(deflate(inputs))\n",
    "        feature_points = inflate(fps, len(inputs))\n",
    "\n",
    "        # storing fps and sites for first image of each snippet (avoiding duplicates)\n",
    "        track_fps.append(feature_points[:, 0])\n",
    "        track_kps.append(sites[:, 0])\n",
    "\n",
    "    track_fps = torch.cat(track_fps).to('cpu')\n",
    "    track_kps = torch.cat(track_kps).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sites = track_kps.shape[1]\n",
    "n_fps = track_fps.shape[1]\n",
    "\n",
    "pairwise_errors = torch.ones((n_sites, n_fps)) * np.inf\n",
    "regrs = [None] * n_sites\n",
    "closest_fps = [None] * n_sites\n",
    "\n",
    "# compute error for each pair of site and feature point\n",
    "for site in range(n_sites):\n",
    "    for fp in range(n_fps):\n",
    "        regr = AffineTransform()\n",
    "        regr.fit(track_fps[:, fp], track_kps[:, site])  # fit transformation\n",
    "        error = regr.mse(track_fps[:, fp], track_kps[:, site])\n",
    "        if torch.all(pairwise_errors[site, :] >= error):\n",
    "            regrs[site] = regr\n",
    "            closest_fps[site] = fp\n",
    "        pairwise_errors[site, fp] = error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = SAC.load('../logs/rl/PandaPush-custom/2023-03-18--19-49-40--34100344/final_model.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venv = setup_environments(rl_cfg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(seed):\n",
    "    _ = venv.reset()\n",
    "    _ = venv.seed(seed)\n",
    "    action = np.array([[0, 0, 0]])\n",
    "    dones = np.array([False])\n",
    "\n",
    "    image = venv.render()\n",
    "    observations = []\n",
    "\n",
    "    while not np.any(dones):\n",
    "        obs, _, dones, info = venv.step(action)\n",
    "        if np.any(dones):\n",
    "            observations.append(info[0]['terminal_observation'])\n",
    "        else:\n",
    "            observations.append(obs.copy())\n",
    "        obs.pop('keypoints')\n",
    "        action, _ = policy.predict(obs, deterministic=True)\n",
    "\n",
    "    return image, observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectories(ax, image, observations, keypoints):\n",
    "\n",
    "    ax.imshow(antialias(torch.tensor(image / 255.0), 2), interpolation='none')\n",
    "    kps = (np.stack([np.squeeze(observations[i]['keypoints']) for i in range(len(observations))], axis=1) + 1) * 128\n",
    "    fps_orig = np.stack([np.squeeze(observations[i]['feature_points']) for i in range(len(observations))], axis=1)\n",
    "    fps = (fps_orig + 1) * 128\n",
    "\n",
    "    for kp_idx in keypoints:\n",
    "        ax.plot(kps[kp_idx, :, 0], kps[kp_idx, :, 1], color='w', marker='.', lw=1, markersize=2)\n",
    "    for fp_idx in [closest_fps[k] for k in keypoints]:\n",
    "        ax.plot(fps[fp_idx, :, 1], fps[fp_idx, :, 0], color='C3', marker='.', lw=1, markersize=2)\n",
    "        \n",
    "    selected_regrs = [regrs[k] for k in keypoints]\n",
    "    for i, fp_idx in enumerate([closest_fps[k] for k in keypoints]):\n",
    "        fps_t = (selected_regrs[i].transform(torch.tensor(fps_orig[fp_idx])) + 1) * 128\n",
    "        ax.plot(fps_t[:, 1], fps_t[:, 0], color='C0', marker='.', lw=1, markersize=2)\n",
    "\n",
    "    ax.set_xlim(31.5, 223.5)\n",
    "    ax.set_ylim(191.5, -0.5)\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, axes1 = plt.subplots(1, 4)\n",
    "fig2, axes2 = plt.subplots(1, 4)\n",
    "\n",
    "for i, seed in enumerate([13, 3, 4, 14]):\n",
    "    image, observations = run_episode(seed)  # 5, 7, 11, 13, 28\n",
    "\n",
    "    axes1[i] = plot_trajectories(axes1[i], image, observations, [4])\n",
    "    axes1[i].axis('off')\n",
    "\n",
    "    axes2[i] = plot_trajectories(axes2[i], image, observations, [0])\n",
    "    axes2[i].axis('off')\n",
    "\n",
    "fig1.set_size_inches(mm2in(122, 28.67))\n",
    "fig1.savefig('../local/paper/img_trajectories_endeffector.pdf')\n",
    "\n",
    "fig2.set_size_inches(mm2in(122, 28.67))\n",
    "fig2.savefig('../local/paper/img_trajectories_object.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = venv.reset()\n",
    "_ = venv.seed(13)\n",
    "action = np.array([[0, 0, 0]])\n",
    "dones = np.array([False])\n",
    "\n",
    "images = []\n",
    "observations = []\n",
    "\n",
    "while not np.any(dones):\n",
    "    images.append(venv.render())\n",
    "    obs, _, dones, info = venv.step(action)\n",
    "    if np.any(dones):\n",
    "        observations.append(info[0]['terminal_observation'])\n",
    "    else:\n",
    "        observations.append(obs.copy())\n",
    "    obs.pop('keypoints')\n",
    "    action, _ = policy.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(antialias(torch.tensor(images[0] / 255.0), 2), interpolation='none')\n",
    "\n",
    "kps = (np.squeeze(observations[0]['keypoints']) + 1) * 128\n",
    "ax.scatter(kps[[0, 1, 4], 0], kps[[0, 1, 4], 1], color='w', marker='.', s=10)\n",
    "\n",
    "ax.set_xlim(31.5, 223.5)\n",
    "ax.set_ylim(191.5, -0.5)\n",
    "ax.axis('off')\n",
    "\n",
    "fig.set_size_inches(mm2in(122 * 0.15, 122 * 0.15))\n",
    "fig.savefig('../local/paper/img_pandapush_start.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(antialias(torch.tensor(images[6] / 255.0), 2), interpolation='none')\n",
    "\n",
    "# kps = (np.squeeze(observations[5]['keypoints']) + 1) * 128\n",
    "# ax.scatter(kps[[0, 1, 4], 0], kps[[0, 1, 4], 1], color='w', marker='.', lw=1)\n",
    "\n",
    "ax.set_xlim(31.5, 223.5)\n",
    "ax.set_ylim(191.5, -0.5)\n",
    "ax.axis('off')\n",
    "\n",
    "fig.set_size_inches(mm2in(122 * 0.15, 122 * 0.15))\n",
    "fig.savefig('../local/paper/img_pandapush_mid.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(antialias(torch.tensor(images[-1] / 255.0), 2), interpolation='none')\n",
    "\n",
    "# kps = (np.squeeze(observations[-2]['keypoints']) + 1) * 128\n",
    "# ax.scatter(kps[[0, 1, 4], 0], kps[[0, 1, 4], 1], color='w', marker='.', lw=1)\n",
    "\n",
    "ax.set_xlim(31.5, 223.5)\n",
    "ax.set_ylim(191.5, -0.5)\n",
    "ax.axis('off')\n",
    "\n",
    "fig.set_size_inches(mm2in(122 * 0.15, 122 * 0.15))\n",
    "fig.savefig('../local/paper/img_pandapush_end.pdf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.voxels(np.ones([1, 1, 1]), alpha=0.5)\n",
    "ax.scatter3D(*(0.5 * np.ones(3)), color='k', s=50)\n",
    "ax.scatter3D(*np.ones(3), color='C3', s=50)\n",
    "ax.quiver(*(0.5 * np.ones(3)), *(0.5 * np.ones(3)), capstyle='round')\n",
    "ax.axis('off')\n",
    "\n",
    "fig.set_size_inches(mm2in(122 * 0.25, 30.2))\n",
    "# fig.savefig(f'../local/paper/img_offset3d.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-rl-BDjg6v1P",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
